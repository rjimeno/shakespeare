<html>
<body>
<h1>
Shakespeare: A Sample Service
</h1>
To provide a model of how a service would hypothetically be deployed in the Google production environment, let’s look at an example service that interacts with multiple Google technologies. Suppose we want to offer a service that lets you determine where a given word is used throughout all of Shakespeare’s works.
We can divide this system into two parts:
<ul>
• A batch component that reads all of Shakespeare’s texts, creates an index, and writes the index into a Bigtable. This job need only run once, or perhaps very infrequently (as you never know if a new text might be discovered!).
• An application frontend that handles end-user requests. This job is always up, as users in all time zones will want to search in Shakespeare’s books.
</ul>
The batch component is a MapReduce comprising three phases.
The mapping phase reads Shakespeare’s texts and splits them into individual words. This is faster if performed in parallel by multiple workers.
The shuffle phase sorts the tuples by word.
In the reduce phase, a tuple of (word, list of locations) is created.
Each tuple is written to a row in a Bigtable, using the word as the key.
 20 | Chapter 2: The Production Environment at Google, from the Viewpoint of an SRE
Life of a Request
Figure 2-4 shows how a user’s request is serviced: first, the user points their browser to shakespeare.google.com. To obtain the corresponding IP address, the user’s device resolves the address with its DNS server (1). This request ultimately ends up at Goo‐ gle’s DNS server, which talks to GSLB. As GSLB keeps track of traffic load among frontend servers across regions, it picks which server IP address to send to this user.
<img src='https://cdn-images-1.medium.com/max/1200/0*p1f26xhz4OXb9HID.jpg' alt='Figure 2-4.  e life of a request'/>
Figure 2-4.  e life of a request

<br/>

The browser connects to the HTTP server on this IP. This server (named the Google Frontend, or GFE) is a reverse proxy that terminates the TCP connection (2). The GFE looks up which service is required (web search, maps, or—in this case—Shake‐ speare). Again using GSLB, the server finds an available Shakespeare frontend server, and sends that server an RPC containing the HTML request (3).

<br/>

The Shakespeare server analyzes the HTML request and constructs a protobuf con‐ taining the word to look up. The Shakespeare frontend server now needs to contact the Shakespeare backend server: the frontend server contacts GSLB to obtain the BNS address of a suitable and unloaded backend server (4). That Shakespeare backend server now contacts a Bigtable server to obtain the requested data (5).

<br/>

The answer is written to the reply protobuf and returned to the Shakespeare backend server. The backend hands a protobuf containing the results to the Shakespeare frontend server, which assembles the HTML and returns the answer to the user.

<br/>

This entire chain of events is executed in the blink of an eye—just a few hundred mil‐ liseconds! Because many moving parts are involved, there are many potential points of failure; in particular, a failing GSLB would wreak havoc. However, Google’s policies of rigorous testing and careful rollout, in addition to our proactive error recovery methods such as graceful degradation, allow us to deliver the reliable service that our users have come to expect. After all, people regularly use www.google.com to check if their Internet connection is set up correctly.

<br/>

<h2>Job and Data Organization</h2>

Load testing determined that our backend server can handle about 100 queries per second (QPS). Trials performed with a limited set of users lead us to expect a peak load of about 3,470 QPS, so we need at least 35 tasks. However, the following consid‐ erations mean that we need at least 37 tasks in the job, or N + 2:
<ul>
• During updates, one task at a time will be unavailable, leaving 36 tasks.
• A machine failure might occur during a task update, leaving only 35 tasks, just enough to serve peak load.5
</ul>
A closer examination of user traffic shows our peak usage is distributed globally: 1,430 QPS from North America, 290 from South America, 1,400 from Europe and Africa, and 350 from Asia and Australia. Instead of locating all backends at one site, we distribute them across the USA, South America, Europe, and Asia. Allowing for N + 2 redundancy per region means that we end up with 17 tasks in the USA, 16 in Europe, and 6 in Asia. However, we decide to use 4 tasks (instead of 5) in South America, to lower the overhead of N + 2 to N + 1. In this case, we’re willing to toler‐ ate a small risk of higher latency in exchange for lower hardware costs: if GSLB redi‐ rects traffic from one continent to another when our South American datacenter is over capacity, we can save 20% of the resources we’d spend on hardware. In the larger regions, we’ll spread tasks across two or three clusters for extra resiliency.
Because the backends need to contact the Bigtable holding the data, we need to also design this storage element strategically. A backend in Asia contacting a Bigtable in the USA adds a significant amount of latency, so we replicate the Bigtable in each region. Bigtable replication helps us in two ways: it provides resilience should a Bigtable server fail, and it lowers data-access latency. While Bigtable only offers even‐ tual consistency, it isn’t a major problem because we don’t need to update the contents often.
We’ve introduced a lot of terminology here; while you don’t need to remember it all, it’s useful for framing many of the other systems we’ll refer to later.
5 We assume the probability of two simultaneous task failures in our environment is low enough to be negligi‐ ble. Single points of failure, such as top-of-rack switches or power distribution, may make this assumption invalid in other environments.
  22 | Chapter 2: The Production Environment at Google, from the Viewpoint of an SRE
  <body>
  </html>
